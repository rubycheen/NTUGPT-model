{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('document/FAQ.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader, UnstructuredFileLoader\n",
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from unstructured.cleaners.core import remove_punctuation, clean, clean_extra_whitespace\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def is_url(url):\n",
    "  try:\n",
    "    result = urlparse(url)\n",
    "    return all([result.scheme, result.netloc])\n",
    "  except ValueError:\n",
    "    return False\n",
    "  \n",
    "\n",
    "  \n",
    "def generate_document(url):\n",
    "    \n",
    "    fake_head = {\n",
    "              'User-Agent': 'My User Agent 1.0',\n",
    "              \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*\"\n",
    "              \";q=0.8\",\n",
    "              \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "              \"Referer\": \"https://www.google.com/\",\n",
    "              \"DNT\": \"1\",\n",
    "              \"Connection\": \"keep-alive\",\n",
    "              \"Upgrade-Insecure-Requests\": \"1\",\n",
    "          }\n",
    "    try:\n",
    "      if is_url(url):\n",
    "          loader = UnstructuredURLLoader(urls=[url],\n",
    "                                        mode=\"elements\",\n",
    "                                        post_processors=[clean,remove_punctuation,clean_extra_whitespace], \n",
    "                                        headers=fake_head)\n",
    "      elif url.endswith('.jpg'):\n",
    "        loader = UnstructuredImageLoader(url,\n",
    "                                        mode=\"elements\",\n",
    "                                        post_processors=[clean,remove_punctuation,clean_extra_whitespace], \n",
    "                                        headers=fake_head)\n",
    "      else:\n",
    "          loader = UnstructuredFileLoader(url, \n",
    "                                  strategy=\"fast\", \n",
    "                                  mode=\"elements\",\n",
    "                                  post_processors=[clean,remove_punctuation,clean_extra_whitespace], \n",
    "                                  headers=fake_head)\n",
    "      elements = loader.load()\n",
    "      # print(f'elements {elements}')\n",
    "      selected_elements = [e for e in elements if e.metadata['category']==\"NarrativeText\" or e.metadata['category']==\"Title\"]\n",
    "      # print(f'selected_elements {selected_elements}')\n",
    "      full_clean = \" \".join([e.page_content for e in selected_elements])\n",
    "      return Document(page_content=full_clean, metadata={\"source\":url})\n",
    "    except:\n",
    "       print(f'*ERROR* {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Updated'] = df['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for idx, item in enumerate(df['Appendix']):\n",
    "    if item is not np.nan:\n",
    "        # print(df['Answer'][idx])\n",
    "        try:\n",
    "            # print(str(df['Answer'][idx])+'\\n'+generate_document(item).page_content)\n",
    "            df['Updated'][idx] = df['Answer'][idx]+generate_document(item).page_content\n",
    "        except:\n",
    "            print(f'*ERROR* @ {df[\"URL\"][idx]}')\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Updated'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Answer'] = df['Updated']\n",
    "df = df.drop(columns=['Updated', 'Appendix'])\n",
    "df.to_csv('FAQ.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_df = pd.read_csv('document/clean_content.csv',encoding='utf-8')\n",
    "\n",
    "train_urls = set(trainset_df['url'])\n",
    "len(trainset_df['url']), len(train_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_df.drop_duplicates(subset=['url'],keep='last',inplace=True,ignore_index=True)\n",
    "# trainset_df = trainset_df.reset_index()\n",
    "len(trainset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_df.to_csv('document/clean_content_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_df = pd.read_csv('document/clean_content_2.csv')\n",
    "len(trainset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for test_url in set(df['URL']):\n",
    "    if test_url not in trainset_df['url']:\n",
    "        # print(test_url, time.time(), generate_document(test_url))\n",
    "        trainset_df.loc[len(trainset_df.index)] = [test_url, time.time(), generate_document(test_url).page_content] \n",
    "len(trainset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_df.to_csv('document/clean_content_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('document/clean_content_3.csv')\n",
    "print(len(df))\n",
    "df.drop_duplicates(subset=['url'],keep='last',inplace=True,ignore_index=True)\n",
    "# trainset_df = trainset_df.reset_index()\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('document/clean_content_4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ChatGPT Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "def ask_chatgpt(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/FAQ.csv')\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "gpts_ans = []\n",
    "for t in tqdm(testset.iterrows()):\n",
    "    text = f\"\"\"\n",
    "    以下是一組問答：\n",
    "    問： {t[1]['Question']}\n",
    "    答：{t[1]['Answer']}\n",
    "    請基於以上問答生成五個意思一樣的問題給我，不需要回答。\n",
    "    問：\"\"\"\n",
    "    token_count = len(encoding.encode(text))\n",
    "    if token_count>4097:\n",
    "        text = f\"\"\"\n",
    "            以下是一組問答：\n",
    "            問： {t[1]['Question']}\n",
    "            答：{t[1]['Answer'][:-(token_count-4097)*2]}\n",
    "            請基於以上問答生成五個意思一樣的問題給我，不需要回答。\n",
    "            問：\n",
    "        \"\"\"\n",
    "        token_count = len(encoding.encode(text))\n",
    "        print(f'After: token_count:{token_count}')\n",
    "    try:\n",
    "        answer = ask_chatgpt(text)['choices'][0]['message']['content']\n",
    "        print('Original answer:', t[1]['Question'])\n",
    "        print('Multi query:', answer)\n",
    "        gpts_ans.append(answer)\n",
    "    except:\n",
    "        print(f'*ERROR* {t} with {len(encoding.encode(text))} tokens.')\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['ChatGPT_MultiQ'] = gpts_ans\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_csv('document/FAQ_ChatGPT_MultiQ.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('document/clean_content_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"content\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs),docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 64\n",
    "CHUNK_OVERLAP = 8\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits), len(all_splits[0].page_content), all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"thenlper/gte-large-zh\")\n",
    "db = FAISS.from_documents(all_splits, embeddings)\n",
    "db.save_local(f'embeddings/faiss_gte_large_{CHUNK_SIZE}_{CHUNK_OVERLAP}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# testset = pd.read_csv('document/FAQ_ChatGPT.csv')\n",
    "testset = pd.read_csv('document/testset.csv')\n",
    "\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_eval(retriever, k):\n",
    "\n",
    "    retrieve_urls = []\n",
    "    \n",
    "    for idx, query in enumerate(testset['Question']):\n",
    "        # print('query:', query)\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        # print('docs:', docs)\n",
    "        retrieve_urls.append(sorted([(doc.metadata['url'])for doc in docs]))\n",
    "    \n",
    "    correct_cnt = 0\n",
    "    for idx, d in enumerate(testset.iterrows()):\n",
    "        if d[1]['URL'] in retrieve_urls[idx]:\n",
    "            correct_cnt+=1\n",
    "    \n",
    "    # print(f\"Correct Answer: {d[1]['URL']}\")\n",
    "    # print(f'Retrieve document urls:')\n",
    "    # [print(url) for url in retrieve_urls[idx]]\n",
    "    print(f'Recall@{k}, ChatGPT Generated Question Accuracy: {correct_cnt/len(testset)}')\n",
    "    return correct_cnt/len(testset)\n",
    "\n",
    "def hyde_retriever_eval(retriever, k):\n",
    "\n",
    "    retrieve_urls = []\n",
    "    \n",
    "    for idx, query in enumerate(testset['Question']):\n",
    "        # print('query:', query)\n",
    "        docs = retriever.get_relevant_documents(query+testset['HyDE'][idx])\n",
    "        # print('docs:', docs)\n",
    "        retrieve_urls.append(sorted([(doc.metadata['url'])for doc in docs]))\n",
    "    \n",
    "    correct_cnt = 0\n",
    "    for idx, d in enumerate(testset.iterrows()):\n",
    "        if d[1]['URL'] in retrieve_urls[idx]:\n",
    "            correct_cnt+=1\n",
    "    \n",
    "    # print(f\"Correct Answer: {d[1]['URL']}\")\n",
    "    # print(f'Retrieve document urls:')\n",
    "    # [print(url) for url in retrieve_urls[idx]]\n",
    "    print(f'Recall@{k}, ChatGPT Generated Question with HyDE Accuracy: {correct_cnt/len(testset)}')\n",
    "    return correct_cnt/len(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retriever_original(retriever, k):\n",
    "\n",
    "#     retrieve_urls = []\n",
    "    \n",
    "#     for idx, query in enumerate(testset['Question']):\n",
    "#         # print('query:', query)\n",
    "#         docs = retriever.get_relevant_documents(query)\n",
    "#         # print('docs:', docs)\n",
    "#         retrieve_urls.append(sorted([(doc.metadata['url'])for doc in docs]))\n",
    "    \n",
    "#     correct_cnt = 0\n",
    "#     for idx, d in enumerate(testset.iterrows()):\n",
    "#         if d[1]['URL'] in retrieve_urls[idx]:\n",
    "#             correct_cnt+=1\n",
    "#     # print(f\"Correct Answer: {d[1]['URL']}\")\n",
    "#     # print(f'Retrieve document urls:')\n",
    "#     # [print(url) for url in retrieve_urls[idx]]\n",
    "#     # print(f'Recall@{k}, Original Question Accuracy: {correct_cnt/len(testset)}')\n",
    "#     return correct_cnt/len(testset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuck Length: BAAI/bge-large-zh-v1.5, chuck by ChatGPT size=64 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"TownsWu/PEG\")\n",
    "db = FAISS.load_local('embeddings/all_PEG_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"infgrad/stella-large-zh\")\n",
    "db = FAISS.load_local('embeddings/all_stella_large_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"infgrad/stella-base-zh\")\n",
    "db = FAISS.load_local('embeddings/all_stella_base_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"yentinglin/bert-base-zhtw\")\n",
    "db = FAISS.load_local('embeddings/all_bert_base_zhtw_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"thenlper/gte-large-zh\")\n",
    "db = FAISS.load_local('embeddings/all_gte_large_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"thenlper/gte-base-zh\")\n",
    "db = FAISS.load_local('embeddings/all_gte_base_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"thenlper/gte-small-zh\")\n",
    "db = FAISS.load_local('embeddings/all_gte_small_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-small-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/all_bge_small_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-base-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/all_bge_base_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/all_bge_large_chatgpt', embeddings)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "# Ks = [1, 5, 10, 20]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)\n",
    "    # hyde_retriever_eval(retriever, K)\n",
    "    # retriever_original(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/faiss_bge_largev1.5_32_8', embeddings)\n",
    "\n",
    "# Ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)\n",
    "    # hyde_retriever_eval(retriever, K)\n",
    "    # retriever_original(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/faiss_bge_largev1.5_64_8', embeddings)\n",
    "\n",
    "# Ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)\n",
    "    # hyde_retriever_eval(retriever, K)\n",
    "    # retriever_original(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/faiss_bge_largev1.5_128_16', embeddings)\n",
    "\n",
    "# Ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)\n",
    "    # hyde_retriever_eval(retriever, K)\n",
    "    # retriever_original(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/faiss_bge_largev1.5_256_16', embeddings)\n",
    "\n",
    "# Ks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K)\n",
    "    # hyde_retriever_eval(retriever, K)\n",
    "    # retriever_original(retriever, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Query x HyDE(5-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "def ask_chatgpt(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/FAQ_ChatGPT.csv')\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['ChatGPT'] = gpts_ans\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_csv('document/FAQ_ChatGPT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('document/clean_content_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"content\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs),docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25: improve a lit bit, but toooo slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/FAQ_ChatGPT.csv')\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only BM25\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('document/ChatGPT_washed_data.csv')\n",
    "loader = DataFrameLoader(df, page_content_column=\"chunk\")\n",
    "docs = loader.load()\n",
    "# CHUNK_SIZE = 64\n",
    "# CHUNK_OVERLAP = 8\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
    "# # )\n",
    "# all_splits = text_splitter.split_documents(docs)\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "Ks = [5, 10, 20, 50, 100]\n",
    "for K in Ks:\n",
    "    # initialize the bm25 retriever and faiss retriever\n",
    "    bm25_retriever.k = K\n",
    "\n",
    "    retriever_eval(bm25_retriever, K)\n",
    "    # hyde_retriever_eval(bm25_retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "embedding = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "faiss_vectorstore = FAISS.load_local('embeddings/all_bge_large_chatgpt', embeddings)\n",
    "\n",
    "\n",
    "df = pd.read_csv('document/ChatGPT_washed_data.csv')\n",
    "loader = DataFrameLoader(df, page_content_column=\"chunk\")\n",
    "docs = loader.load()\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "Ks = [1, 5, 10, 20, 50, 100]\n",
    "for K in Ks:\n",
    "    # initialize the bm25 retriever and faiss retriever\n",
    "    bm25_retriever.k = K\n",
    "    # faiss_vectorstore = FAISS.from_texts(doc_list, embedding)\n",
    "    faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": K})\n",
    "\n",
    "    # initialize the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
    "                                        weights=[0.5, 0.5])\n",
    "\n",
    "    retriever_eval(ensemble_retriever, K)\n",
    "    hyde_retriever_eval(ensemble_retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/FAQ_ChatGPT.csv')\n",
    "testset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARENT_SIZE = 512\n",
    "# CHUNK_SIZE = 64\n",
    "# CHUNK_OVERLAP = 8\n",
    "# parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# vectorstore = Chroma(collection_name=\"split_parents\", persist_directory=f'parent_child_vectorstore_{PARENT_SIZE}', embedding_function=embeddings)\n",
    "\n",
    "# fs = LocalFileStore(f\"kv_docstore_{PARENT_SIZE}\")\n",
    "# store = create_kv_docstore(fs)\n",
    "# Ks = [10, 20, 50, 100]\n",
    "\n",
    "# print(f'Parent Size: {PARENT_SIZE}')\n",
    "# for K in Ks:\n",
    "#     # retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "#     retriever = ParentDocumentRetriever(\n",
    "#         vectorstore=vectorstore,\n",
    "#         docstore=store,\n",
    "#         child_splitter=child_splitter,\n",
    "#         parent_splitter=parent_splitter,\n",
    "#         search_kwargs={\"k\": K}\n",
    "#     )\n",
    "#     print(f'Retrieve: {K} documents to reranking....')\n",
    "#     retriever_eval(retriever, 5)\n",
    "#     hyde_retriever_eval(retriever, 5)\n",
    "#     # retriever_original(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Great!\n",
    "# PARENT_SIZE = 256\n",
    "# CHUNK_SIZE = 64\n",
    "# CHUNK_OVERLAP = 8\n",
    "# parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# vectorstore = Chroma(collection_name=\"split_parents\", persist_directory=f'parent_child_vectorstore_{PARENT_SIZE}', embedding_function=embeddings)\n",
    "\n",
    "# fs = LocalFileStore(f\"kv_docstore_{PARENT_SIZE}\")\n",
    "# store = create_kv_docstore(fs)\n",
    "# Ks = [10, 20, 50, 100]\n",
    "\n",
    "# print(f'Parent Size: {PARENT_SIZE}')\n",
    "# for K in Ks:\n",
    "#     # retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "#     retriever = ParentDocumentRetriever(\n",
    "#         vectorstore=vectorstore,\n",
    "#         docstore=store,\n",
    "#         child_splitter=child_splitter,\n",
    "#         parent_splitter=parent_splitter,\n",
    "#         search_kwargs={\"k\": K}\n",
    "#     )\n",
    "#     print(f'Retrieve: {K} documents to reranking....')\n",
    "#     retriever_eval(retriever, 5)\n",
    "#     hyde_retriever_eval(retriever, 5)\n",
    "#     # retriever_original(retriever, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARENT_SIZE = 128\n",
    "# CHUNK_SIZE = 64\n",
    "# CHUNK_OVERLAP = 8\n",
    "# parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# vectorstore = Chroma(collection_name=\"split_parents\", persist_directory=f'parent_child_vectorstore_{PARENT_SIZE}', embedding_function=embeddings)\n",
    "\n",
    "# fs = LocalFileStore(f\"kv_docstore_{PARENT_SIZE}\")\n",
    "# store = create_kv_docstore(fs)\n",
    "# Ks = [10, 20, 50, 100]\n",
    "# #\n",
    "# print(f'Parent Size: {PARENT_SIZE}')\n",
    "# for K in Ks:\n",
    "#     # retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "#     retriever = ParentDocumentRetriever(\n",
    "#         vectorstore=vectorstore,\n",
    "#         docstore=store,\n",
    "#         child_splitter=child_splitter,\n",
    "#         parent_splitter=parent_splitter,\n",
    "#         search_kwargs={\"k\": K}\n",
    "#     )\n",
    "#     print(f'Retrieve: {K} documents to reranking....')\n",
    "#     retriever_eval(retriever, 5)\n",
    "#     hyde_retriever_eval(retriever, 5)\n",
    "#     # retriever_original(retriever, K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# testset = pd.read_csv('document/FAQ_ChatGPT.csv')\n",
    "testset = pd.read_csv('document/FAQ_filter_merged.csv')\n",
    "\n",
    "testset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc = pd.read_csv('document/clean_content_4.csv', index_col=['url'])\n",
    "# full_doc['url'] = full_doc.index\n",
    "\n",
    "full_doc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def retriever_eval(retriever, k, parent_size, reranker_K=5):\n",
    "\n",
    "    retrieve_urls = []\n",
    "    docs_list = []\n",
    "    docs_passage = []\n",
    "    \n",
    "    cnt = 0\n",
    "    for idx, query in enumerate(testset['Question_ChatGPT_generated']):\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        # print(docs)\n",
    "        \n",
    "        window_passage = []\n",
    "        for doc in docs:\n",
    "            # print(doc)\n",
    "            try:\n",
    "                full_passage = full_doc['content'][doc.metadata['url']]\n",
    "                half_window = (parent_size-len(doc.page_content))//2\n",
    "\n",
    "                str_idx = full_passage.find(doc.page_content)\n",
    "                if str_idx != -1:\n",
    "                    start_idx = str_idx-half_window if str_idx-half_window>=0 else 0\n",
    "                    end_idx = str_idx+half_window if str_idx+half_window<len(full_passage) else len(full_passage)-1\n",
    "\n",
    "                    window_passage.append(full_passage[start_idx:end_idx])\n",
    "                else:\n",
    "                    window_passage.append(full_passage)\n",
    "            except:\n",
    "                    window_passage.append(doc)\n",
    "        docs_passage.append(window_passage)\n",
    "        # docs_list.append(docs)\n",
    "        retrieve_urls.append([(doc.metadata['url'])for doc in docs])\n",
    "    # print(len(docs_passage), len(docs_passage[0]))\n",
    "    # print(len(retrieve_urls),len(retrieve_urls[0]))\n",
    "    # return\n",
    "\n",
    "    correct_cnt = 0\n",
    "    for idx, d in enumerate(testset.iterrows()):\n",
    "        if d[1]['URL'] in retrieve_urls[idx]:\n",
    "            correct_cnt+=1\n",
    "    print(f'Recall@{k}, ChatGPT Generated Question Accuracy: {correct_cnt/len(testset)}')\n",
    "    \n",
    "    if k <= reranker_K:\n",
    "        return correct_cnt/len(testset), docs_list\n",
    "\n",
    "    reranker = FlagReranker('thenlper/gte-base-zh', use_fp16=True)\n",
    "    reranker_index = []\n",
    "    for idx, doc in enumerate(docs_passage): \n",
    "        scores = []\n",
    "        for idxs in range(len(doc)):\n",
    "            try:\n",
    "                # print(query, doc[idxs])\n",
    "                scores.append(reranker.compute_score([query, doc[idxs]]))\n",
    "            except:\n",
    "                scores.append(-np.inf)\n",
    "                print(f'[ERROR] the {idx} document: {doc[idxs]}')\n",
    "                print(f'Check {retrieve_urls[idx][idxs]} in database.')\n",
    "\n",
    "            # print(f'scores: {scores}')\n",
    "        # reranker_scores.append(scores)\n",
    "        # print(f'np.argsort(scores)[::-1]: {np.argsort(scores)[::-1]}')\n",
    "        reranker_index.append(np.argsort(scores)[::-1])\n",
    "    # print(len(reranker_index),len(reranker_index[0]))\n",
    "    correct_cnt = 0\n",
    "    for doc_idx, urls in enumerate(retrieve_urls):\n",
    "        parent_retrieve_urls = []\n",
    "        for idxs in reranker_index[doc_idx][:reranker_K]:\n",
    "            # print(doc[idxs].metadata['url'])\n",
    "            # print(f'doc {doc}, idxs {idxs}')\n",
    "            parent_retrieve_urls.append(urls[idxs])\n",
    "        # print(testset['URL'][doc_idx], retrieve_urls)\n",
    "        # print(testset['ChatGPT'][doc_idx], doc[reranker_index[doc_idx][0]])\n",
    "        if testset['URL'][doc_idx] in parent_retrieve_urls:\n",
    "            correct_cnt+=1\n",
    "    print(f'[Rerank from {k}] Recall@{reranker_K}, ChatGPT Generated Question Accuracy: {correct_cnt/len(testset)}')\n",
    "    return correct_cnt/len(testset), docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "Ks = [5, 10, 20, 50, 100, 200]\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "db = FAISS.load_local('embeddings/all_bge_large_chatgpt', embeddings)\n",
    "\n",
    "print('Parent passage chuck length: 128')\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "Ks = [5, 10, 20, 50, 100, 200]\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "\n",
    "db = FAISS.load_local('embeddings/faiss_bge_largev1.5_64_8', embeddings)\n",
    "\n",
    "print('Parent passage chuck length: 64')\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "Ks = [5, 10, 20, 50, 100, 200]\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "\n",
    "db = FAISS.load_local('embeddings/faiss_bge_largev1.5_64_8', embeddings)\n",
    "\n",
    "print('Parent passage chuck length: 128')\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 50, 100, 200]\n",
    "print('Parent passage chuck length: 256')\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 200})\n",
    "retriever_eval(retriever, 200, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parent passage chuck length: 512')\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parent passage chuck length: 1024')\n",
    "\n",
    "for K in Ks:\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "    retriever_eval(retriever, K, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# from langchain.storage._lc_store import create_kv_docstore\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "\n",
    "# PARENT_SIZE = 1024\n",
    "# CHUNK_SIZE = 64\n",
    "# CHUNK_OVERLAP = 8\n",
    "# parent_splitter = RecursiveCharacterTextSplitter(chunk_size=PARENT_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "# vectorstore = Chroma(collection_name=\"split_parents\", persist_directory=f'parent_child_vectorstore_{PARENT_SIZE}', embedding_function=embeddings)\n",
    "\n",
    "# fs = LocalFileStore(f\"kv_docstore_{PARENT_SIZE}\")\n",
    "# store = create_kv_docstore(fs)\n",
    "# Ks = [5, 20, 100, 200]\n",
    "\n",
    "# # db = FAISS.load_local('embeddings/faiss_bge_largev1.5_64_8', embeddings)\n",
    "\n",
    "# print(f'Parent Size: {PARENT_SIZE}')\n",
    "# for K in Ks:\n",
    "#     # retriever = db.as_retriever(search_kwargs={\"k\": K})\n",
    "#     retriever = ParentDocumentRetriever(\n",
    "#         vectorstore=vectorstore,\n",
    "#         docstore=store,\n",
    "#         child_splitter=child_splitter,\n",
    "#         parent_splitter=parent_splitter,\n",
    "#         search_kwargs={\"k\": K}\n",
    "#     )\n",
    "#     print(f'Retrieve {K} documents to reranking....')\n",
    "#     retriever_eval(retriever, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever1_urls = []\n",
    "# retriever1_ans = []\n",
    "# retriever5_urls = []\n",
    "# retriever1 = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "# retriever5 = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# for idx, query in enumerate(testset['Question']):\n",
    "#     print('Answer:',testset['URL'][idx])\n",
    "#     print('Question:',query)\n",
    "#     docs1 = retriever1.get_relevant_documents(query)\n",
    "#     print(sorted([(doc.metadata['url'])for doc in docs1]))\n",
    "#     print(sorted([(doc.page_content)for doc in docs1]))\n",
    "#     retriever1_urls.append(sorted([(doc.metadata['url'])for doc in docs1]))\n",
    "#     retriever1_ans.append(sorted([(doc.page_content)for doc in docs1]))\n",
    "#     print('======================================================================================')\n",
    "#     docs5 = retriever5.get_relevant_documents(query)\n",
    "#     print(sorted([(doc.metadata['url'])for doc in docs5]))\n",
    "#     retriever5_urls.append(sorted([(doc.metadata['url'])for doc in docs5]))\n",
    "\n",
    "# answer_df = testset\n",
    "# answer_df['retreival doc'] = retriever1_ans\n",
    "# answer_df['retreival url'] = retriever1_urls\n",
    "\n",
    "# answer_df.to_csv('evaluation/retrieval_original.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hyde for Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/testset.csv')\n",
    "testset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"yentinglin/Taiwan-LLM-7B-v2.1-chat\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def generate_text(prompt_text):\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    outputs = llm.generate(inputs[\"input_ids\"], pad_token_id=50256, max_new_tokens=512)\n",
    "    response = tokenizer.decode(outputs[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "response = []\n",
    "for query in tqdm(testset['Question']):\n",
    "    res = generate_text(query)\n",
    "    print(query)\n",
    "    print(res[len(query):])\n",
    "    response.append(res[len(query):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['Answer_Taiwan_Llama_7B_RAG'] = response\n",
    "# testset = testset.drop(columns=['Unnamed: 0'])\n",
    "testset.to_csv('evaluation/testset_Taiwan_Llama_7B_v2.1_RAG.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "569ebef1ef7711f136d82f1147e387bf7a453dccc6cb43bc3c16f0eb4125826b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
