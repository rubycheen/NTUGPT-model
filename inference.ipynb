{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Retrieval (w/o RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/testset.csv')\n",
    "# testset = testset.drop(columns=['Unnamed: 0'])\n",
    "testset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# openai.api_key = \"\"\n",
    "\n",
    "# def ask_chatgpt(prompt):\n",
    "    \n",
    "#     completion = openai.ChatCompletion.create(\n",
    "#       model=\"gpt-3.5-turbo\",\n",
    "#       temperature=0,\n",
    "#       messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are an artificial intelligence assistant and National Taiwan University campus guide\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ]\n",
    "#     )\n",
    "#     return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_id = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # attn_implementation=\"flash_attention_2\" # optional\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# def generate_text(prompt_text):\n",
    "\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"你是一個人工智慧助理以及台灣大學校園導覽員\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": prompt_text},\n",
    "#     ]\n",
    "#     prompt = tokenizer.apply_chat_template(messages)\n",
    "#     print(f'Prompt:{prompt}')\n",
    "#     outputs = llm(prompt)\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import VLLM\n",
    "# import torch\n",
    "# # from transformers import pipeline\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# model_id = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
    "\n",
    "# llm = VLLM(\n",
    "#     model=model_id,\n",
    "#     gpu_memory_utilization=0.95,\n",
    "#     max_new_tokens=512,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     enforce_eager=True,\n",
    "#     max_seq_len=14000,\n",
    "#     trust_remote_code=True,  # mandatory for hf models\n",
    "#     max_model_len=14000,\n",
    "#     # attn_implementation=\"flas\n",
    "#     # h_attention_2\", # optional\n",
    "#     # top_k=50,\n",
    "#     # top_p=0.95,\n",
    "#     # temperature=0.7,\n",
    "#     # gpu_memory_utilization=0.8\n",
    "# )\n",
    "\n",
    "\n",
    "# def generate_text(prompt_text):\n",
    "\n",
    "#     # pipe = pipeline(\"text-generation\", device=0)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#     # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"你是一個人工智慧助理以及台灣大學校園導覽員\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": prompt_text},\n",
    "#     ]\n",
    "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     print(f'Prompt:{prompt}')\n",
    "#     outputs = llm(prompt)\n",
    "\n",
    "#     # print(outputs[0][\"generated_text\"])\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# model_id = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
    "# model_id = \"yentinglin/Taiwan-LLM-13B-v2.0-chat\"\n",
    "model_id = \"taide/TAIDE-LX-7B-Chat\"\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "#     # attn_implementation=\"flash_attention_2\" # optional\n",
    "# ).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\" # optional\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def generate_text(prompt_text):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"你是一個人工智慧助理以及台灣大學校園導覽員\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt_text},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    # print(f'Prompt:{prompt}')\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    outputs = llm.generate(**inputs, max_new_tokens=512)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_id = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
    "\n",
    "# # llm = AutoModelForCausalLM.from_pretrained(\n",
    "# #     model_id,\n",
    "# #     device_map=\"auto\",\n",
    "#     # torch_dtype=torch.bfloat16,\n",
    "# #     # attn_implementation=\"flash_attention_2\" # optional\n",
    "# # ).cuda()\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # load_in_8bit=True,\n",
    "#     # attn_implementation=\"flash_attention_2\" # optional\n",
    "# ).cuda()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# def generate_text(prompt_text):\n",
    "\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"你是一個人工智慧助理以及台灣大學校園導覽員\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": prompt_text},\n",
    "#     ]\n",
    "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "#     print(f'Prompt:{prompt}')\n",
    "\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "\n",
    "#     outputs = llm.generate(**inputs, max_new_tokens=512)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "responses = []\n",
    "for query in tqdm(testset['Question']):\n",
    "    # print(query)\n",
    "    # try:\n",
    "        # response = ask_chatgpt(query)['choices'][0]['message']['content']\n",
    "        # response = generate_text(query)\n",
    "    # except:\n",
    "        # time.sleep(5)\n",
    "        # response = ask_chatgpt(query)['choices'][0]['message']['content']\n",
    "    response = generate_text(query)\n",
    "    print(response)\n",
    "    \n",
    "    responses.append(response)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset[f'Answer_{model_id.split(\"/\")[-1]}'] = responses\n",
    "testset.to_csv(f'evaluation/testset_{model_id.split(\"/\")[-1]}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv('document/testset.csv')\n",
    "testset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_page = pd.read_csv('document/clean_content_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# openai.api_key = \"\"\n",
    "\n",
    "# def ask_chatgpt(prompt):\n",
    "    \n",
    "#     completion = openai.ChatCompletion.create(\n",
    "#       model=\"gpt-3.5-turbo\",\n",
    "#       temperature=0,\n",
    "#       messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are an artificial intelligence assistant and National Taiwan University campus guide\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ]\n",
    "#     )\n",
    "#     return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\").eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# def generate_text(prompt_text):\n",
    "\n",
    "#     inputs = tokenizer(prompt_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "#     outputs = llm.generate(inputs[\"input_ids\"], pad_token_id=50256, max_new_tokens=512)\n",
    "#     response = tokenizer.decode(outputs[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install transformers>=4.34\n",
    "# # pip install accelerate\n",
    "\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "\n",
    "# model_id = \"yentinglin/Taiwan-LLM-7B-v2.1-chat\"\n",
    "\n",
    "# def generate_text(prompt_text):\n",
    "\n",
    "#     pipe = pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "#     # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"你是一個工智慧助理以及台灣大學校園導覽員\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": prompt_text},\n",
    "#     ]\n",
    "#     prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "#     # print(outputs[0][\"generated_text\"])\n",
    "#     return outputs[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.llms import VLLM\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# model_id = \"yentinglin/Taiwan-LLM-7B-v2.1-chat\"\n",
    "\n",
    "# llm = VLLM(\n",
    "#     model=model_id,\n",
    "#     # trust_remote_code=True,  # mandatory for hf models\n",
    "#     max_new_tokens=512,\n",
    "#     top_k=50,\n",
    "#     top_p=0.95,\n",
    "#     temperature=0.7,\n",
    "#     # enforce_eager=True\n",
    "# )\n",
    "\n",
    "\n",
    "# def generate_text(prompt_text):\n",
    "\n",
    "#     pipe = pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "#     # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"你是一個人工智慧助理以及台灣大學校園導覽員\",\n",
    "#         },\n",
    "#         {\"role\": \"user\", \"content\": prompt_text},\n",
    "#     ]\n",
    "#     prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     print(f'Prompt:{prompt}')\n",
    "#     outputs = llm(prompt)\n",
    "\n",
    "#     # print(outputs[0][\"generated_text\"])\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# model_id = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
    "# model_id = \"yentinglin/Taiwan-LLM-13B-v2.0-chat\"\n",
    "model_id = \"taide/TAIDE-LX-7B-Chat\"\n",
    "\n",
    "\n",
    "# llm = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "#     # attn_implementation=\"flash_attention_2\" # optional\n",
    "# ).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\" # optional\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def generate_text(prompt_text):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"你是一個人工智慧助理以及台灣大學校園導覽員\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt_text},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    # print(f'Prompt:{prompt}')\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    outputs = llm.generate(**inputs, max_new_tokens=512)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = \"BAAI/bge-large-zh-v1.5\")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "# relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.55)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[redundant_filter]\n",
    ")\n",
    "\n",
    "db = FAISS.load_local('embeddings/all_bge_large_chatgpt', embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "responses, retrv_urls, retrv_origin_docs, retrv_expand_docs = [], [], [], []\n",
    "\n",
    "for ii, query in enumerate(tqdm(testset['Question'])):\n",
    "    # print(f'Question: {query}')\n",
    "    retrieve_docs = compression_retriever.get_relevant_documents(query)\n",
    "    retrv_origin_docs.append([doc.page_content for doc in retrieve_docs])\n",
    "    url_set = set([doc.metadata['url'] for doc in retrieve_docs])\n",
    "    retrv_urls.append(url_set)\n",
    "\n",
    "    contents = {}\n",
    "    for url in url_set:\n",
    "        for idx, u in enumerate(web_page['url']):\n",
    "            if url==u:\n",
    "                contents[url] = web_page['content'][idx]\n",
    "\n",
    "    paragraphs = []\n",
    "    for i in range(len(retrieve_docs)):\n",
    "        snippet = retrieve_docs[i].page_content\n",
    "        str_idx = contents[retrieve_docs[i].metadata['url']].find(snippet)\n",
    "        if str_idx==-1:\n",
    "            paragraphs.append(snippet)\n",
    "        else:\n",
    "            paragraphs.append(contents[retrieve_docs[i].metadata['url']][str_idx:str_idx+128])\n",
    "    retrv_expand_docs.append(paragraphs)\n",
    "\n",
    "    paragraph = [f'\\n文檔{i+1}:'+paragraphs[i]+'' for i in range(len(paragraphs))]\n",
    "# 回答時不要列出參考資料且\n",
    "    prompt = '以下是參考資料，請忽略不相關的文件，回答盡量簡短精要，切勿重複輸出一樣文句子:{}\\n請問:{}'.format(','.join(paragraph), query)\n",
    "    # prompt = f'你是人工智慧助理，輸出繁體中文，你要對用戶的問題提供警慎且正確、安全、禮貌且簡潔精確的回答，以下是用戶和人工智能助理之間的對話。\\nUSER: {prompt_template} \\nASSISTANT:'\n",
    "    # prompt = prompt_template\n",
    "\n",
    "    # print('Prompt:')\n",
    "    # print(prompt)\n",
    "\n",
    "    # try:\n",
    "    response = generate_text(prompt)\n",
    "    # except:\n",
    "        # time.sleep(5)\n",
    "        # response = generate_text(prompt)['choices'][0]['message']['content']\n",
    "\n",
    "    # response = generate_text(prompt)\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "\n",
    "    print('Ground Truth:')\n",
    "    print(testset['Answer'][ii])\n",
    "    print('\\n*************************************************')\n",
    "\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses,  retrv_urls, retrv_origin_docs, retrv_expand_docs\n",
    "testset[f'Answer_{model_id.split(\"/\")[-1]}_RAG']=responses\n",
    "testset['Retrieve_Expand_Content']=retrv_expand_docs\n",
    "testset['Retrieve_Snippet_Content']=retrv_origin_docs\n",
    "testset['Retreive_Urls']=retrv_urls\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.to_csv(f'evaluation/testset_{model_id.split(\"/\")[1]}_RAG.csv',index=False)\n",
    "testset.to_csv(f'evaluation/testset_{model_id.split(\"/\")[-1]}_RAG.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "569ebef1ef7711f136d82f1147e387bf7a453dccc6cb43bc3c16f0eb4125826b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
